{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence to Sequence Models\n",
    "Data: http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = 'data/spa-eng/spa.txt'\n",
    "with open(txt) as f:\n",
    "    lines = f.read().split('\\n')[:-1]\n",
    "\n",
    "textPairs = []\n",
    "for line in lines:\n",
    "    english, spanish = line.split('\\t')\n",
    "    spanish = '[start] ' + spanish + '[stop]'\n",
    "    textPairs.append((english, spanish))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(textPairs)\n",
    "numValSamp = int(0.15 * len(textPairs))\n",
    "numTrainSamp = len(textPairs) - 2 * numValSamp\n",
    "trainPairs = textPairs[:numTrainSamp]\n",
    "valPairs = textPairs[numTrainSamp: numTrainSamp + numValSamp]\n",
    "testPairs = textPairs[numTrainSamp + numValSamp:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import string\n",
    "import re\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "stripChars = string.punctuation + 'Â¿'\n",
    "stripChars = stripChars.replace('[', '')\n",
    "stripChars = stripChars.replace(']', '')\n",
    "\n",
    "def custom_text_filter(stringIn):\n",
    "    lowercase = tf.strings.lower(stringIn)\n",
    "    return tf.strings.regex_replace(lowercase, f'[{re.escape(stripChars)}]', '')\n",
    "\n",
    "vocabSize = 15000\n",
    "sequenceLen = 20\n",
    "\n",
    "sourceVectorisation = layers.TextVectorization(\n",
    "    max_tokens=vocabSize,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequenceLen,\n",
    ")\n",
    "targetVectorisation = layers.TextVectorization(\n",
    "    max_tokens=vocabSize,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequenceLen + 1,\n",
    "    standardize=custom_text_filter,\n",
    ")\n",
    "trainEnglishTexts = [pair[0] for pair in trainPairs]\n",
    "trainSpanishTexts = [pair[1] for pair in trainPairs]\n",
    "sourceVectorisation.adapt(trainEnglishTexts)\n",
    "targetVectorisation.adapt(trainSpanishTexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training data\n",
    "batchSize = 64\n",
    "\n",
    "def format_dataset(eng, spa):\n",
    "    eng = sourceVectorisation(eng)\n",
    "    spa = targetVectorisation(spa)\n",
    "    return (\n",
    "        {'english': eng, 'spanish': spa[:, :-1]},\n",
    "        spa[:, 1:])  # target Spanish sequence is one step ahead\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    engTexts, spaTexts = zip(*pairs)\n",
    "    engTexts = list(engTexts)\n",
    "    spaTexts = list(spaTexts)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((engTexts, spaTexts))\n",
    "    dataset = dataset.batch(batchSize)\n",
    "    dataset = dataset.map(format_dataset, num_parallel_calls=6)\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()\n",
    "\n",
    "trainDs = make_dataset(trainPairs)\n",
    "valDs = make_dataset(valPairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input English shape: (64, 20)\n",
      "input Spanish shape: (64, 20)\n",
      "targets Spanish shape: (64, 20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-22 21:41:00.023211: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "# check database structure\n",
    "for inputs, targets in trainDs.take(1):\n",
    "    print('input English shape: {}'.format(inputs['english'].shape))\n",
    "    print('input Spanish shape: {}'.format(inputs['spanish'].shape))\n",
    "    print('targets Spanish shape: {}'.format(targets.shape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence to Sequence Using RNN\n",
    "RNN were dominant seq to seq models from 2015 to 2017, then overtaken by transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "embed_dim = 256\n",
    "latent_dim = 1024\n",
    "\n",
    "source = keras.Input(shape=(None,), dtype='int64', name='english')\n",
    "x = layers.Embedding(vocabSize, embed_dim, mask_zero=True)(source)\n",
    "encoded_source = layers.Bidirectional(layers.GRU(latent_dim), merge_mode='sum')(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_target = keras.Input(shape=(None,), dtype='int64', name='spanish')\n",
    "x = layers.Embedding(vocabSize, embed_dim, mask_zero=True)(past_target)\n",
    "\n",
    "decoder_gru = layers.GRU(latent_dim, return_sequences=True)\n",
    "x = decoder_gru(x, initial_state=encoded_source)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "target_next_step = layers.Dense(vocabSize, activation='softmax')(x)  # predict next token\n",
    "seq2seq_rnn = keras.Model([source, past_target], target_next_step)\n",
    "\n",
    "# RNN loks token from 0 to N to predict next token, which is shifted by 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq_rnn.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "seq2seq_rnn.fit(trainDs, epochs=15, validation_data=valDs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erthenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
